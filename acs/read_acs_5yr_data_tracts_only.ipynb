{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: qa\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "debug = 2\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import geopandas\n",
    "import os\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "# Category should be one of: \n",
    "##  demographics, ancestry, birthplace, residence, transportation, household, education...\n",
    "##  language, poverty, disability, income, veteran, employment, housing, health, qa\n",
    "\n",
    "category = 'qa'\n",
    "for_cra_analysis_mac_varlist = ['B01001_001', 'B08013_001', 'B02001_003', 'B02001_004', 'B02001_008', 'B03001_003', 'B15001_001', 'B17001_001', 'B19013_001', 'B19062_001']\n",
    "\n",
    "needed_variables = ['FILEID', 'FILETYPE', 'STUSAB', 'CHARITER', 'SEQUENCE', 'LOGRECNO', 'B00001_001', 'B00002_001']\n",
    "needed_variables = needed_variables + for_cra_analysis_mac_varlist\n",
    "\n",
    "\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "rawdatadir = basedir + 'rawdata/'\n",
    "metadir = basedir + 'metadata/xls_temp/'\n",
    "\n",
    "estimates_dir = basedir + 'estimates/'\n",
    "margin_of_error_dir = basedir + 'margin_of_error/'\n",
    "vardir = basedir + 'variables/'\n",
    "\n",
    "geodir = basedir + 'geography/'\n",
    "#shapefiledir = basedir + 'geography/TRACT_SHAPEFILES/'\n",
    "\n",
    "for thisdir in [estimates_dir, margin_of_error_dir, vardir, geodir]:#[datadir, errordir, vardir, geodir]:\n",
    "    if not(os.path.exists(thisdir)):\n",
    "        os.makedirs(thisdir)\n",
    "        \n",
    "state_codes_df = pandas.read_csv('extras/statecodes.csv')\n",
    "state_codes_df = state_codes_df.set_index('STUSAB')\n",
    "#print('found state codes')\n",
    "if (debug >=  1):\n",
    "    print('Category: {0}'.format(category))\n",
    "print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing estimates for sequence 1...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 2...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 118...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 119...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 120...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 121...\n",
      "Merging datasets...\n",
      "Importing estimates for sequence 122...\n",
      "Merging datasets...\n",
      "backing up...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "estimates_df = pandas.DataFrame()\n",
    "metadata_df = pandas.DataFrame()\n",
    "want_sequences = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    want_sequences.append(i)  \n",
    "\n",
    "if (category == 'for_cra_analysis_mac'):\n",
    "    for i in [4, 5, 23, 43, 48, 59, 63]:\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'demographics'):\n",
    "    for i in range(3,6):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'ancestry'):\n",
    "    for i in range(6,8):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'birthplace'):\n",
    "    for i in range(8,16):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'residence'):\n",
    "    for i in range(16,23):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'transportation'):\n",
    "    for i in range(23,34):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'household'):\n",
    "    for i in range(34,41):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'education'):\n",
    "    for i in range(41,45):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'language'):\n",
    "    for i in range(45,48):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'poverty'):\n",
    "    for i in range(48,57):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'disability'):\n",
    "    for i in range(57,59):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'income'):\n",
    "    for i in range(59,73):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'veteran'):\n",
    "    for i in range(73,75):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'employment'):\n",
    "    for i in range(75,103):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'housing'):\n",
    "    for i in range(103,114):\n",
    "        want_sequences.append(i)     \n",
    "elif (category == 'health'):\n",
    "    for i in range(114,118):\n",
    "        want_sequences.append(i)     \n",
    "elif (category == 'qa'):\n",
    "    for i in range(118,123):\n",
    "        want_sequences.append(i)     \n",
    "\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "for i in want_sequences:\n",
    "    if (debug >= 1):\n",
    "        print('Importing estimates for sequence {0:,.0f}...'.format(i))\n",
    "\n",
    "    this_seq_metadata_filename = metadir + 'seq{0:.0f}.xlsx'.format(i)\n",
    "    this_seq_metadata_in_cols_df = pandas.read_excel(this_seq_metadata_filename, header=None, encoding='utf-8')\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_in_cols_df.columns = this_seq_metadata_in_cols_df.loc[0]\n",
    "\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_df.columns = ['variable', 'description']\n",
    "    \n",
    "    if (i == 1):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = np.nan)\n",
    "        this_seq_metadata_df['sequence_number'] = [0,0,0,0,0,0,1,1]\n",
    "    else:\n",
    "        this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = i)\n",
    "    \n",
    "    this_seq_estimates_df = pandas.DataFrame()\n",
    "    for onestate in states:\n",
    "        statefilename = rawdatadir + 'e20175{1:s}{2:04d}000.txt'.format(i,onestate,i)\n",
    "        onestate_df = pandas.read_csv(statefilename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_estimates_df = this_seq_estimates_df.append(onestate_df)\n",
    "    \n",
    "    this_seq_estimates_df.columns = this_seq_metadata_in_cols_df.columns   \n",
    "    \n",
    "    if (i >= 2):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.drop(['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO'], axis=0)\n",
    "        this_seq_estimates_df = this_seq_estimates_df.drop(['FILEID','FILETYPE','STUSAB', 'CHARITER', 'LOGRECNO'], axis=1)\n",
    "\n",
    "    if (debug >= 2):\n",
    "        print('Merging datasets...')\n",
    "    metadata_df = pandas.concat((metadata_df, this_seq_metadata_df), axis=0)\n",
    "    estimates_df = pandas.concat((estimates_df, this_seq_estimates_df), axis=1)\n",
    "\n",
    "estimates_df['FILETYPE'] = estimates_df['FILETYPE'].astype('str')\n",
    "estimates_df['STUSAB'] = estimates_df['STUSAB'].astype('str')\n",
    "\n",
    "\n",
    "#print('Keeping only needed variables...')\n",
    "#data_df = data_df[needed_variables]\n",
    "#metadata_df = metadata_df.loc[needed_variables]\n",
    "\n",
    "print('backing up...')\n",
    "estimates_df_bk = estimates_df\n",
    "print('Done!')\n",
    "#metadata_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing margin of error values for sequence 1...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 2...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 118...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 119...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 120...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 121...\n",
      "Merging datasets...\n",
      "Importing margin of error values for sequence 122...\n",
      "Merging datasets...\n",
      "backing up...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "margin_of_error_df = pandas.DataFrame()\n",
    "metadata_df = pandas.DataFrame()\n",
    "want_sequences = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    want_sequences.append(i)  \n",
    "\n",
    "if (category == 'for_cra_analysis_mac'):\n",
    "    for i in [4, 5, 23, 43, 48, 59, 63]:\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'demographics'):\n",
    "    for i in range(3,6):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'ancestry'):\n",
    "    for i in range(6,8):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'birthplace'):\n",
    "    for i in range(8,16):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'residence'):\n",
    "    for i in range(16,23):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'transportation'):\n",
    "    for i in range(23,34):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'household'):\n",
    "    for i in range(34,41):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'education'):\n",
    "    for i in range(41,45):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'language'):\n",
    "    for i in range(45,48):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'poverty'):\n",
    "    for i in range(48,57):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'disability'):\n",
    "    for i in range(57,59):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'income'):\n",
    "    for i in range(59,73):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'veteran'):\n",
    "    for i in range(73,75):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'employment'):\n",
    "    for i in range(75,103):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'housing'):\n",
    "    for i in range(103,114):\n",
    "        want_sequences.append(i)     \n",
    "elif (category == 'health'):\n",
    "    for i in range(114,118):\n",
    "        want_sequences.append(i)     \n",
    "elif (category == 'qa'):\n",
    "    for i in range(118,123):\n",
    "        want_sequences.append(i)     \n",
    "\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "for i in want_sequences:\n",
    "    if (debug >= 1):\n",
    "        print('Importing margin of error values for sequence {0:,.0f}...'.format(i))\n",
    "\n",
    "    this_seq_metadata_filename = metadir + 'seq{0:.0f}.xlsx'.format(i)\n",
    "    this_seq_metadata_in_cols_df = pandas.read_excel(this_seq_metadata_filename, header=None, encoding='utf-8')\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_in_cols_df.columns = this_seq_metadata_in_cols_df.loc[0]\n",
    "\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_df.columns = ['variable', 'description']\n",
    "    \n",
    "    if (i == 1):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = np.nan)\n",
    "        this_seq_metadata_df['sequence_number'] = [0,0,0,0,0,0,1,1]\n",
    "    else:\n",
    "        this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = i)\n",
    "    \n",
    "    this_seq_margin_of_error_df = pandas.DataFrame()\n",
    "    for onestate in states:\n",
    "        statefilename = rawdatadir + 'm20175{1:s}{2:04d}000.txt'.format(i,onestate,i)\n",
    "        onestate_df = pandas.read_csv(statefilename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_margin_of_error_df = this_seq_margin_of_error_df.append(onestate_df)\n",
    "        \n",
    "    \n",
    "    this_seq_margin_of_error_df.columns = this_seq_metadata_in_cols_df.columns   \n",
    "#    print('this seq error file has {0:,.0f} rows'.format(len(this_seq_error_df)))    \n",
    "#    print(this_seq_error_df.sample(1).T)\n",
    "    \n",
    "    if (i >= 2):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.drop(['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO'], axis=0)\n",
    "        this_seq_margin_of_error_df = this_seq_margin_of_error_df.drop(['FILEID','FILETYPE','STUSAB', 'CHARITER', 'LOGRECNO'], axis=1)\n",
    "\n",
    "    if (debug >= 2):\n",
    "        print('Merging datasets...')\n",
    "    metadata_df = pandas.concat((metadata_df, this_seq_metadata_df), axis=0)\n",
    "    margin_of_error_df = pandas.concat((margin_of_error_df, this_seq_margin_of_error_df), axis=1)\n",
    "\n",
    "#error_df.sample(1).T\n",
    "margin_of_error_df['FILETYPE'] = margin_of_error_df['FILETYPE'].astype('str')\n",
    "margin_of_error_df['STUSAB'] = margin_of_error_df['STUSAB'].astype('str')\n",
    "\n",
    "#print('Keeping only needed variables...')\n",
    "#error_df = error_df[needed_variables]\n",
    "#metadata_df = metadata_df.loc[needed_variables]\n",
    "\n",
    "print('backing up...')\n",
    "margin_of_error_df_bk = margin_of_error_df\n",
    "print('Done!')\n",
    "#margin_of_error_df.sample(1)\n",
    "#metadata_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading geography files...\n",
      "Reading geography for al...\n",
      "Reading geography for ak...\n",
      "Reading geography for az...\n",
      "Reading geography for ar...\n",
      "Reading geography for ca...\n",
      "Reading geography for co...\n",
      "Reading geography for ct...\n",
      "Reading geography for de...\n",
      "Reading geography for dc...\n",
      "Reading geography for fl...\n",
      "Reading geography for ga...\n",
      "Reading geography for hi...\n",
      "Reading geography for id...\n",
      "Reading geography for il...\n",
      "Reading geography for in...\n",
      "Reading geography for ia...\n",
      "Reading geography for ks...\n",
      "Reading geography for ky...\n",
      "Reading geography for la...\n",
      "Reading geography for me...\n",
      "Reading geography for md...\n",
      "Reading geography for ma...\n",
      "Reading geography for mi...\n",
      "Reading geography for mn...\n",
      "Reading geography for ms...\n",
      "Reading geography for mo...\n",
      "Reading geography for mt...\n",
      "Reading geography for ne...\n",
      "Reading geography for nv...\n",
      "Reading geography for nh...\n",
      "Reading geography for nj...\n",
      "Reading geography for nm...\n",
      "Reading geography for ny...\n",
      "Reading geography for nc...\n",
      "Reading geography for nd...\n",
      "Reading geography for oh...\n",
      "Reading geography for ok...\n",
      "Reading geography for or...\n",
      "Reading geography for pa...\n",
      "Reading geography for ri...\n",
      "Reading geography for sc...\n",
      "Reading geography for sd...\n",
      "Reading geography for tn...\n",
      "Reading geography for tx...\n",
      "Reading geography for ut...\n",
      "Reading geography for vt...\n",
      "Reading geography for va...\n",
      "Reading geography for wa...\n",
      "Reading geography for wv...\n",
      "Reading geography for wi...\n",
      "Reading geography for wy...\n",
      "Retaining only tractlevel geographies...\n",
      "backing up...\n",
      "Done in 94.3 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "geo_df = pandas.DataFrame()\n",
    "\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "if (debug >= 1):\n",
    "        print('Reading geography files...')\n",
    "for onestate in states:\n",
    "    filename = geodir+'{0:}.xlsx'.format(onestate)\n",
    "    if (debug == 2):\n",
    "        print('Reading geography for {0:}...'.format(onestate))\n",
    "    this_geo_df = pandas.read_excel(filename)\n",
    "    geo_df = pandas.concat((geo_df, this_geo_df))\n",
    "\n",
    "geo_df = geo_df.assign(STUSAB = geo_df['State'].apply(lambda x: x.lower()))\n",
    "\n",
    "geo_df = geo_df.rename(columns={'Geography ID': 'GEOID'})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Retaining only tractlevel geographies...')\n",
    "##    print('Retaining only tract- and block-group-level geographies...')\n",
    "geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140'))]\n",
    "##geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140')) | (geo_df['GEOID'].apply(lambda x: x[0:3] == '150'))]\n",
    "\n",
    "print('backing up...')\n",
    "geo_df_bk = geo_df\n",
    "\n",
    "##geo_df = geo_df.set_index('GEOID')  #We'll set GEOID as index colum AFTER the merge\n",
    "e = time.time()\n",
    "print('Done in {0:.1f} seconds!'.format(e-s))\n",
    "\n",
    "#geo_df.sample(1)\n",
    "#print('skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting from backup...\n",
      "backing up...\n",
      "Total estimate of B01001_001: 321,004,407\n",
      "Margin of error in B01001_001: 26,621,941\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('getting from backup...')\n",
    "estimates_df = estimates_df_bk\n",
    "margin_of_error_df = margin_of_error_df_bk\n",
    "\n",
    "#.sort_values('Logical Record Number')\n",
    "#data_df[['STUSAB','LOGRECNO']].dtypes #object, int64\n",
    "#geo_df[['STUSAB', 'Logical Record Number']].dtypes\n",
    "estimates_df = estimates_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "estimates_df = estimates_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "\n",
    "margin_of_error_df = margin_of_error_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "margin_of_error_df = margin_of_error_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "\n",
    "print('backing up...')\n",
    "estimates_df_bk = estimates_df\n",
    "margin_of_error_df_bk = margin_of_error_df\n",
    "\n",
    "print('Total estimate of B01001_001: {0:,.0f}'.format(estimates_df['B01001_001'].sum()))\n",
    "print('Margin of error in B01001_001: {0:,.0f}'.format(margin_of_error_df['B01001_001'].sum()))\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data...\n",
      "Estimates written to estimates_acs2017_tract_qa.csv...\n",
      "Margins of error written to margin_of_error_acs2017_tract_qa.csv...\n",
      "Done in 4 minutes 11 seconds!\n"
     ]
    }
   ],
   "source": [
    "#data_df = data_df[varlist]\n",
    "s = time.time()\n",
    "print('Writing data...')\n",
    "\n",
    "estimates_df.to_csv(estimates_dir + 'estimates_acs2017_tract_{0:}.csv'.format(category), encoding='utf-8')\n",
    "print('Estimates written to estimates_acs2017_tract_{0:}.csv...'.format(category))\n",
    "margin_of_error_df.to_csv(margin_of_error_dir + 'margin_of_error_acs2017_tract_{0:}.csv'.format(category), encoding='utf-8')\n",
    "print('Margins of error written to margin_of_error_acs2017_tract_{0:}.csv...'.format(category))\n",
    "\n",
    "metadata_df.to_csv(vardir + 'variables_acs2017_tract_{0:}.csv'.format(category), encoding='utf-8')\n",
    "#geo_df.to_csv(geodir + 'geo_acs2017_tract.csv', encoding='utf-8')\n",
    "e = time.time()\n",
    "print('Done in {0:,.0f} minutes {1:.0f} seconds!'.format(np.round((e-s)/60,0), ((e-s)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
