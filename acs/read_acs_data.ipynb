{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "debug = 1\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "#from SciServer import CasJobs\n",
    "#from pprint import pprint\n",
    "\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/'\n",
    "rawdatadir = basedir + 'rawdata/'\n",
    "metadir = basedir + 'metadata/xls_temp/'\n",
    "\n",
    "datadir = basedir + 'data/'\n",
    "errordir = basedir + 'error/'\n",
    "vardir = basedir + 'variables/'\n",
    "geodir = basedir + 'geography/'\n",
    "\n",
    "for thisdir in [datadir, errordir, vardir, geodir]:\n",
    "    if not(os.path.exists(thisdir)):\n",
    "        os.makedirs(thisdir)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data for sequence 1...\n",
      "Importing data for sequence 139...\n",
      "Importing data for sequence 140...\n",
      "Importing data for sequence 141...\n",
      "Importing data for sequence 142...\n",
      "Importing data for sequence 143...\n",
      "Importing data for sequence 144...\n",
      "Importing data for sequence 145...\n",
      "Importing data for sequence 146...\n",
      "Importing data for sequence 147...\n",
      "Importing data for sequence 148...\n",
      "Importing data for sequence 149...\n",
      "Importing data for sequence 150...\n",
      "Importing data for sequence 151...\n",
      "Importing data for sequence 152...\n",
      "Importing data for sequence 153...\n",
      "Importing data for sequence 154...\n",
      "Importing data for sequence 155...\n",
      "Importing data for sequence 156...\n",
      "Importing data for sequence 157...\n",
      "Importing data for sequence 158...\n",
      "Importing data for sequence 159...\n",
      "Importing data for sequence 160...\n",
      "Importing data for sequence 161...\n"
     ]
    }
   ],
   "source": [
    "# category can be one of: demographics, ancestry, residence, transportation, household, income, employment, housing, qa\n",
    "category = 'housing'\n",
    "\n",
    "states = ['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc']\n",
    "states += ['de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in']\n",
    "states += ['ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn']\n",
    "states += ['mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj']\n",
    "states += ['nm', 'nv', 'ny', 'oh', 'ok', 'or']\n",
    "states += ['pa', 'pr', 'ri', 'sc', 'sd', 'tn', 'tx', 'us']\n",
    "states += ['ut' ,'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
    "\n",
    "data_df = pandas.DataFrame()\n",
    "metadata_df = pandas.DataFrame()\n",
    "error_df = pandas.DataFrame()\n",
    "\n",
    "# 1-6, 139-150\n",
    "want_sequences = []\n",
    "for i in range(1,2):\n",
    "    want_sequences.append(i)\n",
    "\n",
    "if (category == 'demographics'):\n",
    "    for i in range(2,7):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'ancestry'):\n",
    "    for i in range(7,19):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'residence'):\n",
    "    for i in range(19,29):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'transportation'):\n",
    "    for i in range(29,45):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'household'):\n",
    "    for i in range(45,62):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'income'):\n",
    "    for i in range(62,92):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'employment'):\n",
    "    for i in range(92,139):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'housing'):\n",
    "    for i in range(139,162):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'qa'):\n",
    "    for i in range(162,167):\n",
    "        want_sequences.append(i)\n",
    "else:\n",
    "    print('Category {0:} not found!'.format(category))\n",
    "    \n",
    "for i in want_sequences:\n",
    "    if (debug >= 1):\n",
    "#        if ((np.mod(i, 10) == 0) | (i == len(want_sequences))):\n",
    "        print('Importing data for sequence {0:,.0f}...'.format(i))\n",
    "    this_seq_data_df = pandas.DataFrame()\n",
    "    this_seq_metadata_filename = metadir + 'seq{0:.0f}.xlsx'.format(i)\n",
    "#    this_seq_metadata_filename = '/home/idies/workspace/raddick_acs_data/metadata/Seq{0:.0f}.xls'.format(i)\n",
    "    this_seq_metadata_in_cols_df = pandas.read_excel(this_seq_metadata_filename, header=None, encoding='utf-8')\n",
    "    this_seq_metadata_in_cols_df = this_seq_metadata_in_cols_df.dropna(axis=1)\n",
    "    \n",
    "    if (i == 1):\n",
    "        this_seq_metadata_in_cols_df.columns = ['FILEID','FILETYPE','STUSAB','CHARITER','SEQUENCE','LOGRECNO','B00001_001_UW','B00001_002_UW']\n",
    "    else:\n",
    "        this_seq_metadata_in_cols_df.columns = this_seq_metadata_in_cols_df.loc[0]\n",
    "\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_df.columns = ['variable', 'description']\n",
    "    if (i == 1):\n",
    "        this_seq_metadata_df['variable'] = this_seq_metadata_in_cols_df.columns\n",
    "        this_seq_metadata_df['sequence_number'] = [0,0,0,0,0,0,1,1] \n",
    "    else:\n",
    "        this_seq_metadata_df['sequence_number'] = i\n",
    "\n",
    "    this_seq_data_df = pandas.DataFrame()\n",
    "    for onestate in states:\n",
    "        statefilename = rawdatadir + 'e20171{1:s}{2:04d}000.txt'.format(i,onestate,i)\n",
    "        onestate_df = pandas.read_csv(statefilename, header=None, sep=',', encoding='utf-8')\n",
    "        this_seq_data_df = this_seq_data_df.append(onestate_df)\n",
    "\n",
    "    this_seq_data_df.columns = this_seq_metadata_in_cols_df.columns\n",
    "    \n",
    "    this_seq_data_df = this_seq_data_df.drop(['SEQUENCE'], axis=1)\n",
    "\n",
    "    if (i >= 2):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.drop(['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO'], axis=0)\n",
    "        this_seq_data_df = this_seq_data_df.drop(['FILEID','FILETYPE','STUSAB', 'CHARITER', 'LOGRECNO'], axis=1)\n",
    "\n",
    "    if (debug >= 2):\n",
    "        print('Merging datasets...')\n",
    "    metadata_df = pandas.concat((metadata_df, this_seq_metadata_df), axis=0)\n",
    "    data_df = pandas.concat((data_df, this_seq_data_df), axis=1)\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Creating pseudo geoid...')\n",
    "data_df['PSEUDO_GEOID'] = data_df['STUSAB'].str.upper() + data_df['LOGRECNO'].apply(lambda x: '{0:07d}'.format(x))\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Indexing...')\n",
    "data_df = data_df.set_index('PSEUDO_GEOID')\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Reading geography file...')\n",
    "geo_metadata_in_cols_df = pandas.read_excel(metadir + '2016_SFGeoFileTemplate_maybe_2017.xls', header=None, encoding='utf-8')\n",
    "geo_metadata_in_cols_df = geo_metadata_in_cols_df.dropna(axis=1)\n",
    "geo_metadata_in_cols_df.columns = geo_metadata_in_cols_df.loc[0]\n",
    "geo_metadata_in_cols_df.index = ['variable','description']\n",
    "\n",
    "geo_metadata_df = geo_metadata_in_cols_df.T\n",
    "geo_metadata_df.columns = ['variable','description']\n",
    "\n",
    "geo_df = pandas.DataFrame()\n",
    "for onestate in states:\n",
    "    try:\n",
    "        this_geo_df = pandas.read_csv(rawdatadir + 'g20171{0:}.csv'.format(onestate), low_memory=False, encoding='iso-8859-1', header=None)\n",
    "    except UnicodeDecodeError:\n",
    "        print(rawdatadir + 'g20171{0:}.csv not found!'.format(onestate))\n",
    "    geo_df = pandas.concat((geo_df, this_geo_df), axis=0)\n",
    "\n",
    "geo_df.columns = geo_metadata_in_cols_df.columns\n",
    "\n",
    "geo_df['PSEUDO_GEOID'] = geo_df['STUSAB'].str.upper() + geo_df['LOGRECNO'].apply(lambda x: '{0:07d}'.format(x))\n",
    "\n",
    "geo_df = geo_df.set_index('PSEUDO_GEOID')\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Merging geography with data...')\n",
    "\n",
    "data_df = data_df.join(geo_df[['GEOID','NAME']])\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Resetting index...')\n",
    "\n",
    "metadata_df.loc['GEOID'] = ['GEOID', 'Geography identifier (also functions as index of data tables)', 0]\n",
    "metadata_df.loc['NAME'] = ['NAME', 'Name of geography region', 0]\n",
    "\n",
    "data_df = data_df.set_index('GEOID', drop=False)\n",
    "data_df.index.name = ''\n",
    "\n",
    "metadata_df = metadata_df.set_index('variable', drop=False)\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Writing variables file...')\n",
    "\n",
    "if not (os.path.exists(vardir)):\n",
    "    os.makedirs(vardir)\n",
    "if not (os.path.exists(geodir)):\n",
    "    os.makedirs(geodir)\n",
    "\n",
    "metadata_df.to_csv(vardir + 'variables_acs2017_{0:}.csv'.format(category), encoding='utf-8')\n",
    "geo_df.to_csv(geodir + 'geo_acs2017.csv', encoding='utf-8')\n",
    "geo_metadata_df.to_csv(geodir + 'geo_variables_acs2017.csv', encoding='utf-8')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Writing data (estimates) file...')\n",
    "\n",
    "    \n",
    "if not (os.path.exists(datadir)):\n",
    "    os.makedirs(datadir)\n",
    "    \n",
    "data_df.to_csv(datadir + 'data_acs2017_{0:}.csv'.format(category), encoding='utf-8')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "\n",
    "# category can be one of: demographics, ancestry, residence, transportation, household, income, employment, housing, qa\n",
    "category = 'qa'\n",
    "\n",
    "basedir = '/home/idies/workspace/persistent/censusdata/'\n",
    "rawdatadir = basedir + 'rawdata/'\n",
    "metadir = basedir + 'metadata/Templates/'\n",
    "\n",
    "datadir = basedir + 'data/'\n",
    "errordir = basedir + 'error/'\n",
    "vardir = basedir + 'variables/'\n",
    "geodir = basedir + 'geography/'\n",
    "\n",
    "debug = 1\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "#from SciServer import CasJobs\n",
    "#from pprint import pprint\n",
    "\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "states = ['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc']\n",
    "states += ['de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in']\n",
    "states += ['ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn']\n",
    "states += ['mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj']\n",
    "states += ['nm', 'nv', 'ny', 'oh', 'ok', 'or']\n",
    "states += ['pa', 'pr', 'ri', 'sc', 'sd', 'tn', 'tx', 'us']\n",
    "states += ['ut' ,'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
    "\n",
    "data_df = pandas.DataFrame()\n",
    "metadata_df = pandas.DataFrame()\n",
    "error_df = pandas.DataFrame()\n",
    "\n",
    "# 1-6, 139-150\n",
    "want_sequences = []\n",
    "for i in range(1,2):\n",
    "    want_sequences.append(i)\n",
    "\n",
    "if (category == 'demographics'):\n",
    "    for i in range(2,7):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'ancestry'):\n",
    "    for i in range(7,19):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'residence'):\n",
    "    for i in range(19,29):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'transportation'):\n",
    "    for i in range(29,45):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'household'):\n",
    "    for i in range(45,62):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'income'):\n",
    "    for i in range(62,92):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'employment'):\n",
    "    for i in range(92,139):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'housing'):\n",
    "    for i in range(139,162):\n",
    "        want_sequences.append(i)\n",
    "elif (category == 'qa'):\n",
    "    for i in range(162,167):\n",
    "        want_sequences.append(i)\n",
    "else:\n",
    "    print('Category {0:} not found!'.format(category))\n",
    "    \n",
    "for i in want_sequences:\n",
    "    if (debug >= 1):\n",
    "#        if ((np.mod(i, 10) == 0) | (i == len(want_sequences))):\n",
    "        print('Importing data for sequence {0:,.0f}...'.format(i))\n",
    "    this_seq_error_df = pandas.DataFrame()\n",
    "    this_seq_metadata_filename = metadir + 'seq{0:.0f}.xlsx'.format(i)\n",
    "#    this_seq_metadata_filename = '/home/idies/workspace/raddick_acs_data/metadata/Seq{0:.0f}.xls'.format(i)\n",
    "    this_seq_metadata_in_cols_df = pandas.read_excel(this_seq_metadata_filename, header=None, encoding='utf-8')\n",
    "    this_seq_metadata_in_cols_df = this_seq_metadata_in_cols_df.dropna(axis=1)\n",
    "    \n",
    "    if (i == 1):\n",
    "        this_seq_metadata_in_cols_df.columns = ['FILEID','FILETYPE','STUSAB','CHARITER','SEQUENCE','LOGRECNO','B00001_001_UW','B00001_002_UW']\n",
    "    else:\n",
    "        this_seq_metadata_in_cols_df.columns = this_seq_metadata_in_cols_df.loc[0]\n",
    "\n",
    "    this_seq_metadata_df = this_seq_metadata_in_cols_df.T\n",
    "    this_seq_metadata_df.columns = ['variable', 'description']\n",
    "    if (i == 1):\n",
    "        this_seq_metadata_df['variable'] = this_seq_metadata_in_cols_df.columns\n",
    "        this_seq_metadata_df['sequence_number'] = [0,0,0,0,0,0,1,1] \n",
    "    else:\n",
    "        this_seq_metadata_df['sequence_number'] = i\n",
    "\n",
    "    for onestate in states:\n",
    "        statefilename = rawdatadir + 'e20171{1:s}{2:04d}000.txt'.format(i,onestate,i)\n",
    "        onestate_df = pandas.read_csv(statefilename, header=None, sep=',', encoding='utf-8')\n",
    "        this_seq_error_df = this_seq_error_df.append(onestate_df)\n",
    "\n",
    "    this_seq_error_df.columns = this_seq_metadata_in_cols_df.columns\n",
    "    \n",
    "    this_seq_error_df = this_seq_error_df.drop(['SEQUENCE'], axis=1)\n",
    "\n",
    "    if (i >= 2):\n",
    "        this_seq_metadata_df = this_seq_metadata_df.drop(['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO'], axis=0)\n",
    "        this_seq_error_df = this_seq_error_df.drop(['FILEID','FILETYPE','STUSAB', 'CHARITER', 'LOGRECNO'], axis=1)\n",
    "\n",
    "    if (debug >= 2):\n",
    "        print('Merging datasets...')\n",
    "    metadata_df = pandas.concat((metadata_df, this_seq_metadata_df), axis=0)\n",
    "    error_df = pandas.concat((error_df, this_seq_error_df), axis=1)\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Creating pseudo geoid...')\n",
    "error_df['PSEUDO_GEOID'] = error_df['STUSAB'].str.upper() + error_df['LOGRECNO'].apply(lambda x: '{0:07d}'.format(x))\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Indexing...')\n",
    "error_df = error_df.set_index('PSEUDO_GEOID')\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Reading geography file...')\n",
    "geo_metadata_in_cols_df = pandas.read_excel(metadir + '2016_SFGeoFileTemplate_maybe_2017.xls', header=None, encoding='utf-8')\n",
    "geo_metadata_in_cols_df = geo_metadata_in_cols_df.dropna(axis=1)\n",
    "geo_metadata_in_cols_df.columns = geo_metadata_in_cols_df.loc[0]\n",
    "geo_metadata_in_cols_df.index = ['variable','description']\n",
    "\n",
    "geo_metadata_df = geo_metadata_in_cols_df.T\n",
    "geo_metadata_df.columns = ['variable','description']\n",
    "\n",
    "geo_df = pandas.DataFrame()\n",
    "for onestate in states:\n",
    "    try:\n",
    "        this_geo_df = pandas.read_csv(rawdatadir + 'g20171{0:}.csv'.format(onestate), low_memory=False, encoding='iso-8859-1', header=None)\n",
    "    except UnicodeDecodeError:\n",
    "        print(rawdatadir + 'g20171{0:}.csv not found!'.format(onestate))\n",
    "    geo_df = pandas.concat((geo_df, this_geo_df), axis=0)\n",
    "\n",
    "geo_df.columns = geo_metadata_in_cols_df.columns\n",
    "\n",
    "geo_df['PSEUDO_GEOID'] = geo_df['STUSAB'].str.upper() + geo_df['LOGRECNO'].apply(lambda x: '{0:07d}'.format(x))\n",
    "\n",
    "geo_df = geo_df.set_index('PSEUDO_GEOID')\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Merging geography with data...')\n",
    "\n",
    "error_df = error_df.join(geo_df[['GEOID','NAME']])\n",
    "\n",
    "if (debug >= 2):\n",
    "    print('Resetting index...')\n",
    "\n",
    "metadata_df.loc['GEOID'] = ['GEOID', 'Geography identifier (also functions as index of data tables)', 0]\n",
    "metadata_df.loc['NAME'] = ['NAME', 'Name of geography region', 0]\n",
    "\n",
    "error_df = error_df.set_index('GEOID', drop=False)\n",
    "error_df.index.name = ''\n",
    "\n",
    "metadata_df = metadata_df.set_index('variable', drop=False)\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Writing variables file...')\n",
    "\n",
    "if not (os.path.exists(vardir)):\n",
    "    os.makedirs(vardir)\n",
    "if not (os.path.exists(geodir)):\n",
    "    os.makedirs(geodir)\n",
    "\n",
    "metadata_df.to_csv(vardir + 'variables_acs2017_{0:}.csv'.format(category), encoding='utf-8')\n",
    "geo_df.to_csv(geodir + 'geo_acs2016.csv', encoding='utf-8')\n",
    "geo_metadata_df.to_csv(geodir + 'geo_variables_acs2017.csv', encoding='utf-8')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Writing margins of error file...')\n",
    "\n",
    "    \n",
    "if not (os.path.exists(errordir)):\n",
    "    os.makedirs(errordir)\n",
    "    \n",
    "error_df.to_csv(errordir + 'error_acs2017_{0:}.csv'.format(category), encoding='utf-8')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reorder columns\n",
    "#new_column_list = []\n",
    "#new_column_list.append(geo_metadata_df.columns[len(geo_metadata_df.columns)-5])\n",
    "#for i in range(0,len(geo_metadata_df.columns)-5):\n",
    "#    new_column_list.append(geo_metadata_df.columns[i])\n",
    "#for j in range(len(geo_metadata_df.columns)-4, len(geo_metadata_df.columns)):\n",
    "#    new_column_list.append(geo_metadata_df.columns[j])\n",
    "#geo_metadata_df.columns = new_column_list\n",
    "\n",
    "#new_column_list = []\n",
    "#new_column_list.append(geo_df.columns[len(geo_df.columns)-5])\n",
    "#for i in range(0,len(geo_df.columns)-5):\n",
    "#    new_column_list.append(geo_df.columns[i])\n",
    "#for j in range(len(geo_df.columns)-4, len(geo_df.columns)):\n",
    "#    new_column_list.append(geo_df.columns[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Females under age 5 in Baltimore County: {0:,.0f} with margin of error {1:,.0f}'.format(data_df['B01001_027'].loc['05000US24510'], error_df['B01001_027'].loc['05000US24510']))\n",
    "#data_df['B01001_027'].loc['05000US24510']\n",
    "#error_df['B01001_027'].loc['05000US24510']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
